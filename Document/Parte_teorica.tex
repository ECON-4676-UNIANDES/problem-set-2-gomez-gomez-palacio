\documentclass[12pt,onecolumn]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          				PACKAGES  				              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[margin=1.5in]{geometry}
\usepackage{authblk}
%\usepackage[latin1]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{placeins}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{a4wide,graphicx,color}
\usepackage[colorlinks=true,linkcolor=black,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage[table]{xcolor}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{color,soul}
\usepackage{threeparttable}
\usepackage[capposition=top]{floatrow}
\usepackage[labelsep=period]{caption}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lscape}
\usepackage{pdflscape}
\usepackage{multicol}
\usepackage[bottom]{footmisc}
\setlength\footnotemargin{5pt}
\usepackage{longtable}
\usepackage{chronosys}
\usepackage{physics}
\catcode`\@=11
\def\chron@selectmonth#1{\ifcase#1\or Jan\or Feb\or Mar\or Apr\or May\or Jun\or Jul\or Aug\or Sep\or Oct\or Nov\or Dec\fi}

%% BibTeX settings
\usepackage{natbib}
\bibliographystyle{apalike}
\bibpunct{(}{)}{,}{a}{,}{,}

%% markup commands for code/software
\let\code=\texttt
\let\pkg=\textbf
\let\proglang=\textsf
\newcommand{\file}[1]{`\code{#1}'}
\newcommand{\email}[1]{\href{mailto:#1}{\normalfont\texttt{#1}}}
\urlstyle{same}

%% paragraph formatting
\renewcommand{\baselinestretch}{1}

%% \usepackage{Sweave} is essentially
\RequirePackage[T1]{fontenc}
\RequirePackage{ae,fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}

% Defines columns for tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}


\usepackage{bbm}
\usepackage{enumitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%             Commands           %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     			TITLE, AUTHORS AND DATE    			  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Problem Set 2}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Econ 4676: Big Data and Machine Learning for Applied Economics}
\author{Daniel Gómez Salazar\thanks{d.gomezs@uniandes.edu.co}, Lucas Gómez Tobón\thanks{l.gomezt@uniandes.edu.co}, José Daniel Palacio Murillo \thanks{jd.palacio@uniandes.edu.co}}
\date{}

\begin{document}
\maketitle

\section{Theory Exercises}

\begin{enumerate}
    \item Suppose you have the following spatial model $y=\rho W y + X\beta + WX\theta  +\epsilon$ with $|\rho|<1$  this is sometimes known as the Spatial Durbin Model
    \begin{enumerate}
      \item First consider the following scenario  $\beta=\theta=0$. 
        \begin{enumerate}
            \item \bf{Write the Likelihood function. Can you find a closed form for the parameter estimators? Don't forget to be specific on the assumptions you make.}
        \end{enumerate}

        \begin{equation*}
            y=\rho{Wy}+\epsilon
        \end{equation*}

        We take into consideration the following asumptions:

        \begin{itemize}
            \item $\lvert \rho \rvert < 1$
            \item $\epsilon \sim N(0,\sigma^2)$
            \item w is exogenous
        \end{itemize}

        With this being said we can now define $y$:
        \begin{equation*}
            \begin{split}
                y-\rho{Wy}=\epsilon \\
                (I_n-\rho{W})y=\epsilon \\
                y=(I_n-\rho{W})^{-1}\epsilon
            \end{split}
        \end{equation*}
        Therefore, the likelihood function can be defined as:
        \begin{equation*}
            L(y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{det(v(y))\rvert}^{-\frac{1}{2}}}exp\left({-\frac{1}{2}}{\left(y-E(y)\right)'}{v(y)^{-1}}{\left(y-E(y)\right)}\right)
        \end{equation*}
        To fin the MLE we need $E[y]$ and $v(y)$:
        \begin{equation*}
            \begin{split}
                E[y]=E\left[(I_n-\rho{W})^{-1}E(\epsilon)\right] \\
                \text{we assume that $\rho$ is given and $\epsilon \distras{i.i.d} N(0,\sigma^2)$} \\
                \Rightarrow E[y]=0
            \end{split}    
        \end{equation*}
        Now we find $v(y)$
        \begin{equation*}
            \begin{split}
                v(y)=E[yy']-E[y]E[y]' \\
                E[yy']=E\left[\left(\left(I_n-\rho{W}\right)^{-1}\epsilon\right)\cdot\left(\left(I_n-\rho{W}\right)^{-1}\epsilon\right)'\right] \\
                E[yy']=E\left[\left(I_n-\rho{W}\right)^{-1}\epsilon\epsilon'\left(\left(I_n-\rho{W}\right)^{-1}\right)'\right] \\
                E[yy']=E\left[\left(I_n-\rho{W}\right)^{-1}\left(\left(I_n-\rho{W}\right)^{-1}\right)'\sigma^2\right] \\
                E[yy']=\underbrace{\left[(I_n-\rho{W})'(I_n-\rho{W})\right]^{-1}}_\text{$\Omega$}\sigma^2
            \end{split}
        \end{equation*}
        \begin{equation*}
            v(y)=\sigma^2\Omega
        \end{equation*}
        Now, it is possible to define the likelihood function:
        \begin{equation*}
            L(\rho,\sigma^2,y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{\sigma^2\Omega\rvert}^{-\frac{1}{2}}}\cdot{exp\left({-\frac{1}{2}}{\left(y-0\right)'}{\left(\sigma^2\Omega\right)^{-1}}{\left(y-0\right)}\right)}
        \end{equation*}
        \begin{equation*}
            L(\rho,\sigma^2,y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{\sigma^2\Omega\rvert}^{-\frac{1}{2}}}\cdot{exp\left({-\frac{1}{2\sigma^2}}{\left((I_n-\rho{W})^{-1}\epsilon\right)'}{\Omega^{-1}}{\left((I_n-\rho{W})^{-1}\epsilon\right)}\right)} 
        \end{equation*}
        \begin{equation*}
            \begin{split}
                L(\rho,\sigma^2,y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{\sigma^2\Omega\rvert}^{-\frac{1}{2}}}\cdot\\
                {exp\left({-\frac{1}{2\sigma^2}}\left(\epsilon'\underbrace{\left(I_n-\rho{W})'\right)^{-1}(I_n-\rho{W})'}_\text{I}\underbrace{(I_n-\rho{W})(I_n-\rho{W})^{-1}}_\text{I}\epsilon\right)\right)} \\
            \end{split}
        \end{equation*}
        \begin{equation*}
            L(\rho,\sigma^2,y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{\sigma^2\Omega\rvert}^{-\frac{1}{2}}}\cdot{exp\left({-\frac{1}{2\sigma^2}}\epsilon'\epsilon\right)}
        \end{equation*}
        
        We use the log function:
        \begin{equation*}
            l(\sigma^2,\rho,y)={-\frac{n}{2}}\ln(2\pi)-{\frac{1}{2}}\ln\left(\lvert\sigma^2\Omega\rvert\right)-{\frac{1}{2\sigma^2}}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
        \end{equation*}
        note that $\lvert\sigma^2\Omega\rvert=\sigma^{2n}\lvert\Omega\rvert$, also $\lvert\Omega\rvert=\lvert(I_n-\rho{W})\rvert^{-2}$
        \begin{equation*}
            \begin{split}
            l(\rho,\sigma^2,y)={-\frac{n}{2}}\ln(2\pi)-{\frac{1}{2}}(n)\ln(\sigma^2)-{\frac{n}{2}}(-2)\ln(\lvert{I-\rho{W}}\rvert) \\
            -{\frac{1}{2\sigma^2}}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
            \end{split}
        \end{equation*}
        \begin{equation*}
            l(\rho,\sigma^2,y)=-{\frac{n}{2}}ln(2\pi)-{\frac{n}{2}}ln(\sigma^2)+n\ln(\lvert{I-\rho{W}}\rvert)-{\frac{1}{2\sigma^2}}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
        \end{equation*}
        Ord (1975) showed that
        \begin{equation*}
            \lvert{I-\rho{W}}\rvert=\prod_{i=1}^{n}(1-\rho{W_i}), \text{where $W_i$ is the eigenvalue of i.}
        \end{equation*}
        \begin{equation*}
            l(\rho,\sigma^2,y)=-{\frac{n}{2}}ln(2\pi)-{\frac{n}{2}}ln(\sigma^2)+n\Sigma\ln(1-\rho{W_i})-{\frac{1}{2\sigma^2}}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
        \end{equation*}
        \begin{equation*}
            l(\sigma^2,y|\rho)=-{\frac{n}{2}}ln(2\pi)-{\frac{n}{2}}ln(\sigma^2)+n\Sigma\ln(1-\rho{W_i})-{\frac{1}{2\sigma^2}}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
        \end{equation*}
        \begin{equation*}
            \bullet \quad \pdv{l}{\sigma^2}=-{\frac{n}{2}\frac{1}{\sigma^2}}+\frac{1}{2}\frac{1}{\sigma^4}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y=0
        \end{equation*}
        \begin{equation*}
            \frac{1}{2}\frac{1}{\sigma^4}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y={\frac{n}{2}\frac{1}{\sigma^2}}
        \end{equation*}
        \begin{equation*}
            \boxed{\sigma^2(\rho)=\frac{1}{n}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y}=\frac{\epsilon'\epsilon}{n}
        \end{equation*}
        \begin{equation*}
            \bullet \quad \pdv{l}{\rho}=n\Sigma\frac{-W_i}{1-\rho{W_i}}+\frac{1}{\sigma^2}\left(y'(I-\rho{W})'Wy\right)=0
        \end{equation*}
        Since $\rho$ cannot be derived analytically, $\rho$ mmust be obtained from an explicit maximization of a concentrated log-likelihood function using numerical optimization:
        \begin{equation*}
            l(\rho)=-{\frac{n}{2}}ln(2\pi)-{\frac{n}{2}}ln(\sigma^2(\rho))+n\Sigma\ln(1-\rho{W_i})-{\frac{1}{2\sigma^2(\rho)}}\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
        \end{equation*}
        \begin{equation*}
            \sigma^2_{ML}(\hat{\rho})=\frac{1}{n}\left((I_n-\hat{\rho}{W})y\right)'(I_n-\hat{\rho}{W})y
        \end{equation*}
        \begin{enumerate}[resume]
            \item \bf{Suppose instead you use MCO, would you obtain the same estimates?}
        \end{enumerate}
        \begin{equation*}
            y=(I_n-\rho{W})^{-1}\epsilon
        \end{equation*}
        We now minimize the squared error:
        \begin{equation*}
            \underset{\rho}{min} \quad e'e=\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y
        \end{equation*}
        Is not possible to fin a closed form for the estimates $\hat{\rho}_{OLS}$ and $\hat{\sigma}^2_{OLS}$. Therefore:
        \begin{equation*}
            \hat{\sigma}^2_{OLS}\neq\hat{\sigma}^2_{ML} \quad ; \quad \hat{\rho}_{OLS}\neq\hat{\rho}_{ML}
        \end{equation*}
        \begin{equation*}
            \left[\rho\right]: \quad -2y'(I-\hat{\rho}W)'Wy=0
        \end{equation*}
        \begin{equation*}
            y'(I-\hat{\rho}W')Wy=0
        \end{equation*}
        \begin{equation*}
            (y'-\hat{\rho}y'W')Wy=0
        \end{equation*}
        \begin{equation*}
            y'Wy-\hat{\rho}y'W'Wy=0
        \end{equation*}
        \begin{equation*}
            \hat{\rho}y'W'Wy=y'Wy
        \end{equation*}
        \begin{equation*}
            \hat{\rho}_{OLS}=y'Wy(y'W'Wy)^{-1}
        \end{equation*}
        We also have:
        \begin{equation*}
            \hat{\sigma}^2_{OLS}=\frac{e'e}{n-k}=\frac{\left((I_n-\rho{W})y\right)'(I_n-\rho{W})y}{n-k}\neq\hat{\sigma}^2_{ML}
        \end{equation*}
      \item Now consider that $\rho=0$, and let's proceed as before:
        \begin{enumerate}
            \item \bf{Write the Likelihood function. Can you find a closed form for the parameter estimators? Don't forget to be specific on the assumptions you make.}
        \end{enumerate}
        \begin{equation*}
            y=X{\beta}+WX\theta+\epsilon; \quad \epsilon \distras{i.i.d}N(0,\sigma^2)
        \end{equation*}
        \begin{equation*}
            y=z\gamma+\epsilon; \quad z=[X,WX] \text{and} \gamma=[\beta,\theta]
        \end{equation*}
        Therefore:
        \begin{equation*}
            L(y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{det(v(y))\rvert}^{-\frac{1}{2}}}exp\left({-\frac{1}{2}}{\left(y-E(y)\right)'}{v(y)^{-1}}{\left(y-E(y)\right)}\right)
        \end{equation*}
        \begin{equation*}
            L(y)=\left(\frac{1}{2\pi}\right)^\frac{n}{2}\cdot{\lvert{det(v(y))\rvert}^{-\frac{1}{2}}}exp\left({-\frac{1}{2}}{\left(y-z\gamma\right)'}{v(y)^{-1}}{\left(y-z\gamma\right)}\right)
        \end{equation*}
        \begin{equation*}
            \Rightarrow l(y)={-\frac{n}{2}}\ln(2\pi)-{\frac{n}{2}}\ln(\sigma^2))-{\frac{1}{2\sigma^2}}(y-z\gamma)'(y-z\gamma)
        \end{equation*}
        \begin{equation*}
            \pdv{l}{\sigma^2}=-{\frac{n}{2}\frac{1}{\sigma^2}}+\frac{1}{2}\frac{1}{\sigma^4}(y-z\gamma)'(y-z\gamma)=0
        \end{equation*}
        \begin{equation*}
            \Rightarrow \hat{\sigma}^2=\frac{1}{n}(y-z\gamma)'(y-z\gamma)
        \end{equation*}
        Concentrated likelihood
        \begin{equation*}
            l^c(y|\gamma,z)=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}ln\left(\frac{1}{n}(y-z\gamma)'(y-z\gamma)\right)-\frac{n}{2}
        \end{equation*}
        \begin{equation*}
            \left[\gamma\right]: \quad \frac{n}{(y-z\hat{\gamma})'(y-z\hat{\gamma})}\cdot{z'(y-z\hat{\gamma})}=0
        \end{equation*}
        \begin{equation*}
            z'y=z'z\hat{\gamma}
        \end{equation*}
        \begin{equation*}
            \boxed{\hat{\gamma_{ML}}=(z'z)^{-1}(z'y)}
        \end{equation*}
        \begin{equation*}
            \boxed{\hat{\sigma_{ML}}^2=\frac{1}{n}\left(y-z(z'z)^{-1}(z'y)\right)'\left(y-z(z'z)^{-1}(z'y)\right)}=\frac{\epsilon'\epsilon}{n}
        \end{equation*}
        \begin{enumerate}[resume]
            \item \bf{Suppose instead you use MCO, would you obtain the same estimates?} 
        \end{enumerate}  
        In that case we have:
        \begin{equation*}
            \underset{\gamma}{min} \quad e'e=(y-z\gamma)'(y-z\gamma)
        \end{equation*}
        \begin{equation*}
            \left[\gamma\right]: \quad -2z'(y-z\gamma)=0
        \end{equation*}
        \begin{equation*}
            \boxed{\hat{\gamma}_{OLS}=(z'z)^{-1}(z'y)} \Rightarrow \quad \hat{\gamma}_{ML}=\hat{\gamma}_{OLS}
        \end{equation*}
        \begin{equation*}
            \boxed{\hat{\sigma}^2_{OLS}=\frac{\epsilon'\epsilon}{n-k-1}} \Rightarrow \quad \hat{\sigma}^2_{ML}\neq\hat{\sigma}^2_{OLS}
        \end{equation*}
        And it can be proved as it follows:
        \begin{equation*}
            y=z\gamma+\epsilon \Rightarrow \quad \epsilon=y-z\gamma
        \end{equation*}
       \begin{equation*}
           \hat{\epsilon}=y-z\hat{\gamma} \Rightarrow \quad \hat{\epsilon}=y-z(z'z)^{-1}z'y \Rightarrow \quad \hat{\epsilon}=(I-z(z'z^{-1}z)y) 
       \end{equation*}
       \begin{equation*}
           \hat{\epsilon}=My, \text{where $M$ is an idempotent matrix}
       \end{equation*} 
       \begin{equation*}
           var(\epsilon)=E\left[(\epsilon-E[\epsilon])'(\epsilon-E[\epsilon])\right]=E[\epsilon'\epsilon], \text{since $E(\epsilon)=0$}
       \end{equation*}
       \begin{equation*}
           E[\epsilon'\epsilon|z]=E[y'M'My|z]=E[y'My|z]
       \end{equation*}
       The scalar $\epsilon'M\epsilon$ is a $1x1$ matrix, so its equal to its trace. By using the result on cyclic perutations
       \begin{equation*}
           E[tr(\epsilon'M\epsilon)|z]=E[tr(M\epsilon\epsilon')|z]
       \end{equation*}
       Since $M$ is function of $z$:
       \begin{equation*}
           tr\left(E[\epsilon\epsilon'|z]\right)=tr(M\sigma^2I)=\sigma^2tr(M)
       \end{equation*}
       \begin{equation*}
           tr\left((I-z(z'z)^{-1}z)\right)=tr(I)-tr(z(z'z)^{-1}z)=n-k-1
       \end{equation*}
       \begin{equation*}
           E[\epsilon'\epsilon|z]=(n-k-1)\sigma^2
       \end{equation*}
       \begin{equation*}
           \Rightarrow \quad \hat{\sigma}^2=\frac{\epsilon'\epsilon}{n-k-1}
       \end{equation*}
    \end{enumerate}  
    \item Consider the regression model $y=X\beta +\epsilon$ with $\epsilon\sim N(0,\sigma^2I)$ furthermore assume that $\beta$ has a normal prior, i.e. $\beta\sim N(0,\tau^2I)$. 
  \begin{enumerate}
      \item \bf{Find the posterior distribution.}
  \end{enumerate}

  The posterior distribution is built as it follows:

  \begin{equation*}
      \pi(\beta|y,X)=\frac{f(y,X|\beta)p(\beta)}{m(y,X)}
  \end{equation*}
  \begin{equation*}
    \pi(\beta|y,X)=\frac{f(y|X,\beta)f(X|\beta)p(\beta)}{m(y,X)}
  \end{equation*}

  With the asumption that $f(X|\beta)=f(x)$:
  \begin{equation*}
      \pi(\beta|y,X)=f(y|X,\beta)p(\beta)\frac{f(X)}{m(y,X)}
  \end{equation*}
  \begin{equation*}
      \pi(\beta|y,X)\propto\underbrace{f(y|X,\beta)}_\text{likelihood}\underbrace{p(\beta)}_\text{prior}
  \end{equation*}

  We have the following distribution for $y$:
  \begin{equation*}
      L(y|\beta,\sigma^2,X)=\prod_{i=1}^{n}\left(\frac{1}{\sqrt{2\pi\sigma^2}}exp\left({-\frac{1}{2\sigma^2}(y_i-x_i\beta)}\right)\right)
  \end{equation*}
  \begin{equation*}
      L(y|\beta,\sigma^2,X)=(2\pi\sigma^2)^{\frac{n}{2}}{exp\left(-\frac{1}{2\sigma^2}(y-X\beta)'(y-X\beta)\right)}
  \end{equation*}

  We also have the prior:
  \begin{equation*}
      p(\beta)=\left(2\pi\tau^2\right)^{-\frac{1}{2}}exp\left(-\frac{1}{2\tau^2}(\beta'\beta)\right)
  \end{equation*}

  Which in turn gives the posterior distribution for $\beta$:
  \begin{equation*}
      \pi(\beta|y,X)\propto(2\pi\sigma^2)^{\frac{n}{2}}{exp\left(-\frac{1}{2\sigma^2}(y-X\beta)'(y-X\beta)\right)}\left(2\pi\tau^2\right)^{-\frac{1}{2}}exp\left(-\frac{1}{2\tau^2}(\beta'\beta)\right)
  \end{equation*}
  \begin{equation*}
      \pi(\beta|y,X)\propto{exp\left(-\frac{1}{2}\left(\frac{1}{\sigma^2}{(y-X\beta)'(y-X\beta)}+\frac{1}{\tau^2}\beta'\beta\right)\right)}
  \end{equation*}
  
  With this given, we can find the following distribution of the posterior:
  \begin{equation*}
      \Sigma=\left(\frac{1}{\sigma^2}X'X+\frac{1}{T}\right), \text{where $T=\tau^2I$}
  \end{equation*}
  \begin{equation*}
      \mu=\frac{1}{\sigma^2}\Sigma{X'y}\Rightarrow\mu=\frac{1}{\sigma^2}\left(\frac{1}{\sigma^2}X'X+\frac{1}{T}\right)X'y
  \end{equation*}
  Implying:
  \begin{equation*}
      \beta|y,X\sim{N}(\mu,\Sigma)
  \end{equation*}

  \begin{enumerate}[resume]   
      \item \bf{Compare it with the ridge formula we saw in class.} 
  \end{enumerate}

  The following formula is the ridge formula for $\hat{\beta}$:
  \begin{equation*}
      \hat{\beta}_R=(X'X+\lambda{I})X'y
  \end{equation*}

  Which is similar to the estimation using a Bayesian estimation approach:

  \begin{equation*}
      \hat{\beta}_{Bayesian}=\frac{1}{\sigma^2}\left(\frac{1}{\sigma^2}X'X+\frac{1}{T}\right)X'y
  \end{equation*}

Their main differences lie in the inclusion of both variance parameters within the Bayesian estimation unlike the Ridge estimator which only includes the Lagrange multiplier.

  \begin{enumerate}[resume]    
      \item \bf{What is the relationship between $\lambda$ in the ridge model and $\sigma^2$ and $\tau^2$?}
  \end{enumerate}
  
  As we see in the previous point, the Bayesian estimation method allows us to see how the shrinkage of the estimation towards the prior or the data depends on the variance observed in the data $\sigma^2$ and the variance of the prior $\tau^2$.

  \begin{equation*}
      \lambda\approx\frac{\sigma^2}{\tau^2}
  \end{equation*}

    \item \bf{Centered Ridge. Suppose that $\bar x= 0$, i.e. the data has been centered. Show that the parameters that minimize $R(\beta,\beta_0) = (y-X\beta-\beta_0 \iota)'(y-X\beta-\beta_0 \iota)+ \lambda\beta'\beta$ are $\beta_0=\bar y$ and $\beta=(X'X+\lambda I)^{-1}X'y$}
\end{enumerate}
We define the model as it follows:
\begin{equation*}
    y=\beta_0\iota+X\beta+\varepsilon
\end{equation*}
where $\iota$ is a n-vector of 1s and X is an $n\times(k-1)$ matrix of observations. This specification would take the variables to the mean, but since the mean for each $x=0$ it doesn't change the specification nor we need to use the $M_\iota$ matrix.
\\
\\
To find the estimator one should solve the least squares problem subject to the restriction in which $\beta'\beta=0$, which can be expressed as a Lagrangian:
\begin{equation*}
    \left(\hat{\beta_0},\hat{\beta}\right)=\underset{\beta_0,\beta}{argmin} \left[(y-X\beta-\beta_o\iota)'(y-X\beta-\beta_o\iota)+\lambda(\beta'\beta)\right]
\end{equation*}
We expand the equation:
\begin{equation*}
    \left(\hat{\beta_0},\hat{\beta}\right)=\underset{\beta_0,\beta}{argmin}\left[y'y-y'X\beta-y'\beta_o\iota-\beta'X'y+\beta'X'X\beta-\iota'\beta_0'y+\iota'\beta_0'\beta_0\iota+\lambda\beta'\beta\right]
\end{equation*}
Now we find the first order conditions that allow us to find the estimators:
\begin{equation*}
    \left[\beta_0\right]: \quad -2\iota'y+2\iota'\iota\beta_0=0
\end{equation*}
\begin{equation*}
    \iota'\iota\beta_0=\iota'y
\end{equation*}
where $\iota'\iota'=1$ and $\iota'y$ calculates the mean of the variable $y$:
\begin{equation*}
    \hat{\beta_0}=\bar{y}
\end{equation*}
\begin{equation*}
    \left[\beta\right]: \quad -2X'y+2(X'X+\lambda{I})\beta=0
\end{equation*}
\begin{equation*}
    (X'X+\lambda{I})\beta=X'y
\end{equation*}
\begin{equation*}
    \hat{\beta}=(X'X+\lambda{I})^{-1}X'y
\end{equation*}
\begin{enumerate}[resume]  
    \item \bf{Suppose that we have the following regression model  $y=X\beta +\epsilon$, and decide to do the following: Augment the centered matrix $X$ with $p$ additional rows with $\sqrt{\lambda}$, and augment $y$ with zeros. Show that this procedures renders the ridge regression estimates, is there a link to the leverage statistic?}
\end{enumerate}

We define the model as it follows in order to find the estimator $\beta$:

\begin{gather}
    \begin{pmatrix}
        y \\
        0
    \end{pmatrix} =
   \begin{pmatrix}
       X \\
       \sqrt{\lambda}
   \end{pmatrix}\beta+\epsilon
\end{gather}

The following equation shows the related estimator for the model:
\begin{gather}
    \hat{\beta}=\left(\begin{pmatrix}
        X&\sqrt{\lambda}
    \end{pmatrix}\begin{pmatrix}
        X \\ \sqrt{\lambda}
    \end{pmatrix}\right)^{-1}
    \left(\begin{pmatrix}
        X & \sqrt{\lambda}
    \end{pmatrix}\begin{pmatrix}
        y \\ 0
    \end{pmatrix}\right)
\end{gather}
\begin{equation*}
    \hat{\beta}=(X'X+\lambda{I})^{-1}X'y
\end{equation*}
One can also take into consideration the $H$ matrix also called the projection matrix which contains in the diagonal the $\lambda$ used to construct the ridge estimator, showing the connection between the leverage statistic and the ridge regression.
\begin{equation*}
    H=X(X'X)^{-1}X'=\begin{pmatrix}
        X \\ \sqrt{\lambda}
    \end{pmatrix}(X'X+\lambda{I})^{-1}\begin{pmatrix}
        X & \sqrt{\lambda}
    \end{pmatrix}
\end{equation*}
leading to the $\hat{y}$:
\begin{equation*}
    \hat{y}=\begin{pmatrix}
        X \\ \sqrt{\lambda}
    \end{pmatrix}(X'X+\lambda{I})X'y
\end{equation*}
Therefore we can assume that the ridge estimator follows the same procedure as the leverage statistic in which we want to reduce the overfitting through introducing new information to the model.
\begin{enumerate}[resume]    
    \item \bf{Reducing elastic net to lasso. Suppose that you have the following functions $EL(\beta) = (y-X\beta)^2+ \lambda_2 \beta^2+ \lambda_1|\beta|$ and $L(\beta) = (\tilde{y}-\tilde{X}\beta)^2+ c \lambda_1|\beta|$ where $c=(1+\lambda_2)^{\frac{-1}{2}}$ show that these two problems are equivalent when $\tilde{y}$ and $\tilde{X}$ are the augmented data versions of the previous exercise.}
\end{enumerate}

\end{document}